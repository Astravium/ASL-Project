{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    #device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    loader = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                with tqdm.tqdm(enumerate(loader[phase]), total=len(loader[phase]), desc=f\"{phase.capitalize()} Epoch {e+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "                    for i, batch in pbar:\n",
    "                        x = batch[0].to(device)\n",
    "                        y = batch[1].to(device)\n",
    "                        output = model(x)\n",
    "\n",
    "                        n = x.shape[0]\n",
    "                        global_step += n\n",
    "                        loss = criterion(output, y)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        accuracy = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
    "                        loss_meter.add(loss.item(), n)\n",
    "                        acc_meter.add(accuracy, n)\n",
    "\n",
    "                        pbar.set_postfix(loss=loss_meter.value(), accuracy=acc_meter.value())\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                            writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('loss/' + phase, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + phase, acc_meter.value(), global_step=global_step)\n",
    "\n",
    "        torch.save(model.state_dict(), '%s-%d.pth' % (exp_name, e+1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageValueMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "        \n",
    "    def add(self, val, n=1):\n",
    "        self.sum += val*n\n",
    "        self.num += n\n",
    "        \n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.C1=nn.Conv2d(1,6,5)\n",
    "        self.S2=nn.AvgPool2d(2)\n",
    "        self.C3=nn.Conv2d(6,16,5)\n",
    "        self.S4=nn.AvgPool2d(2)\n",
    "        # Fully connected layers\n",
    "        self.F5=nn.Linear(256,120)\n",
    "        self.F6=nn.Linear(120,84)\n",
    "        self.F7=nn.Linear(84,22)\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.C1(x)\n",
    "        x=self.S2(x)\n",
    "        x = self.activation(x)\n",
    "        x=self.C3(x)\n",
    "        x=self.S4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.F5(x.view(x.shape[0],-1)) # reshape tensori\n",
    "        x = self.activation(x)\n",
    "        x=self.F6(x)\n",
    "        x = self.activation(x)\n",
    "        x=self.F7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45446"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeNet()\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def split_train_val_test(dataset, perc=None):  # default 60% train, 10% val, 30% test\n",
    "\n",
    "    if perc is None:\n",
    "        perc = [0.6, 0.3]\n",
    "    train, test = train_test_split(dataset, test_size=perc[1], train_size=perc[0])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "sign_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'T', 'U', 'V', 'W', 'X',\n",
    "               'Y']\n",
    "\n",
    "dict_alph = {'a': 0,'b': 1,'c': 2,'d': 3,'e': 4,'f': 5,'h': 6,'i': 7,'k': 8,'l': 9,'m': 10,\n",
    "             'n': 11,'o': 12,'p': 13,'q': 14,'r': 15,'t': 16,'u': 17,'v': 18,'w': 19,'x': 20,'y': 21\n",
    "}\n",
    "\n",
    "DATASET1_FOLDER = \"../data/Dataset_Elvio\"\n",
    "DATASET2_FOLDER = \"../data/Dataset_Manuel\"\n",
    "DATASET3_FOLDER = \"../data/Dataset_Michele\"\n",
    "DATASET4_FOLDER = \"../data/Dataset_Juliana\"\n",
    "\n",
    "DATASET_ESTESO = 'Dataset_michele_esteso/'\n",
    "datasets = [DATASET2_FOLDER]\n",
    "#datasets = [DATASET2_FOLDER, DATASET3_FOLDER, DATASET4_FOLDER,DATASET_ESTESO]\n",
    "#letters = ['a','b','c','d','e', 'f', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 't', 'u', 'v', 'w', 'x', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "DATASET SIZE: 3300\n"
     ]
    }
   ],
   "source": [
    "transform_aug = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.RandomPerspective(0.3,0.2),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "transform_base = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "\n",
    "# DATASET CREATION WITH NEW DATA AUG \n",
    "data = []\n",
    "\n",
    "for letter in dict_alph:\n",
    "    for dataset in datasets:\n",
    "        sub_folder = os.path.join(dataset, letter)\n",
    "        for img_name in os.listdir(sub_folder):\n",
    "            img_path = os.path.join(sub_folder, img_name)\n",
    "            im0 = transform_base(Image.open(img_path))\n",
    "            #im1 = transform_aug(Image.open(img_path))\n",
    "            #im2 = transform_aug(Image.open(img_path))\n",
    "            label = dict_alph[img_name[0]]\n",
    "            data.append((im0, label))\n",
    "            #data.append((im1, label))\n",
    "            #data.append((im2, label))\n",
    "print(data[0][0].shape)  # Should be (1, 64, 64) for grayscale images\n",
    "print(\"DATASET SIZE:\",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_val_test(data, [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train, batch_size=32, num_workers=2, shuffle=True)\n",
    "test_data = DataLoader(test, batch_size=32, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/10: 100%|██████████| 73/73 [00:05<00:00, 12.80batch/s, accuracy=0.045, loss=3.1]  \n",
      "Test Epoch 1/10: 100%|██████████| 31/31 [00:03<00:00,  9.53batch/s, accuracy=0.0535, loss=3.09]\n",
      "Train Epoch 2/10: 100%|██████████| 73/73 [00:04<00:00, 16.93batch/s, accuracy=0.0961, loss=3.04]\n",
      "Test Epoch 2/10: 100%|██████████| 31/31 [00:02<00:00, 10.72batch/s, accuracy=0.413, loss=2.71]\n",
      "Train Epoch 3/10: 100%|██████████| 73/73 [00:04<00:00, 16.42batch/s, accuracy=0.614, loss=1.34]\n",
      "Test Epoch 3/10: 100%|██████████| 31/31 [00:03<00:00, 10.10batch/s, accuracy=0.874, loss=0.387]\n",
      "Train Epoch 4/10: 100%|██████████| 73/73 [00:03<00:00, 18.47batch/s, accuracy=0.931, loss=0.183]\n",
      "Test Epoch 4/10: 100%|██████████| 31/31 [00:03<00:00,  9.20batch/s, accuracy=0.997, loss=0.032] \n",
      "Train Epoch 5/10: 100%|██████████| 73/73 [00:04<00:00, 14.97batch/s, accuracy=0.998, loss=0.0165]\n",
      "Test Epoch 5/10: 100%|██████████| 31/31 [00:03<00:00,  8.23batch/s, accuracy=0.999, loss=0.0072] \n",
      "Train Epoch 6/10: 100%|██████████| 73/73 [00:06<00:00, 11.46batch/s, accuracy=1, loss=0.00256]\n",
      "Test Epoch 6/10: 100%|██████████| 31/31 [00:04<00:00,  7.64batch/s, accuracy=0.998, loss=0.0051] \n",
      "Train Epoch 7/10: 100%|██████████| 73/73 [00:05<00:00, 13.06batch/s, accuracy=1, loss=0.000864]\n",
      "Test Epoch 7/10: 100%|██████████| 31/31 [00:04<00:00,  7.23batch/s, accuracy=0.999, loss=0.00468]\n",
      "Train Epoch 8/10: 100%|██████████| 73/73 [00:04<00:00, 16.48batch/s, accuracy=1, loss=0.00035] \n",
      "Test Epoch 8/10: 100%|██████████| 31/31 [00:04<00:00,  7.49batch/s, accuracy=0.999, loss=0.00352]\n",
      "Train Epoch 9/10: 100%|██████████| 73/73 [00:04<00:00, 15.82batch/s, accuracy=1, loss=0.000196]\n",
      "Test Epoch 9/10: 100%|██████████| 31/31 [00:04<00:00,  6.49batch/s, accuracy=0.999, loss=0.0021] \n",
      "Train Epoch 10/10: 100%|██████████| 73/73 [00:05<00:00, 14.35batch/s, accuracy=1, loss=0.000101]\n",
      "Test Epoch 10/10: 100%|██████████| 31/31 [00:03<00:00,  8.26batch/s, accuracy=0.999, loss=0.00148]\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet()\n",
    "lenet = train_classifier(lenet, train_data, test_data, exp_name='lenet_model', lr=0.01, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test_classifier(model, loader):\n",
    "    #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm.tqdm(loader, desc=\"Testing\") as pbar:\n",
    "            for batch in pbar:\n",
    "                x = batch[0].to(device)\n",
    "                y = batch[1].to(device)\n",
    "                output = model(x)\n",
    "                preds = output.to(\"cpu\").max(1)[1].numpy()\n",
    "                labs = y.to(\"cpu\").numpy()\n",
    "                predictions.extend(list(preds))\n",
    "                labels.extend(list(labs))\n",
    "\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 73/73 [00:09<00:00,  7.41it/s]\n",
      "Testing: 100%|██████████| 31/31 [00:09<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy di training: 1.0000\n",
      "Accuracy di test: 0.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lenet_predictions_train, lenet_labels_train = test_classifier(lenet, train_data)\n",
    "lenet_predictions_test, lenet_labels_test = test_classifier(lenet, test_data)\n",
    "print(\"Accuracy di training: %0.4f\"%(accuracy_score(lenet_labels_train, lenet_predictions_train)))\n",
    "print(\"Accuracy di test: %0.4f\"%(accuracy_score(lenet_labels_test, lenet_predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_error(gt, pred):\n",
    "    return 100*(1-accuracy_score(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore LeNet su DIGITS training: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Errore LeNet su DIGITS training: %0.2f%%\"%(perc_error(lenet_labels_train, lenet_predictions_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lenet, \"lenet_lr01_ep10\" + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LeNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetV2, self).__init__()\n",
    "        # Definiamo il primo livello. Dobbiamo effettuare una convoluzione 2D (ovvero su immagini)\n",
    "        # Utilizziamo il modulo Conv2d che prende in input:\n",
    "        # - il numero di canali in input: 1 (si tratta di immagini in scala di grigio)\n",
    "        # - il numero di canali in output: 6 (le mappe di feature)\n",
    "        # - la dimensione del kernel: 5 (sta per \"5 X 5\")\n",
    "        self.C1 = nn.Conv2d(1, 6, 5)\n",
    "\n",
    "        # Definiamo il livello di subsampling. Questo viene implementato usando il modulo \"MaxPool2d\"\n",
    "        # Il modulo prende in input la dimensione dei neighbourhood rispetto ai quali calcolare i valori massimi: 2\n",
    "        self.S2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Definiamo il livello C3 in maniera analoga a quanto fatto per il livello C1:\n",
    "        self.C3 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        # Definiamo il successivo max pooling 2d\n",
    "        self.S4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Definiamo il primo layer FC\n",
    "        self.F5 = nn.Linear(256, 120)\n",
    "\n",
    "        # Definiamo il secondo Layer FC\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "\n",
    "        # Definiamo il terzo layer FC\n",
    "        self.F7 = nn.Linear(84, 22)\n",
    "\n",
    "        # Definiamo inoltre un modulo per calcolare l'attivazione ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.C1(x)\n",
    "        x = self.S2(x)\n",
    "        x = self.activation(x) # inseriamo le attivazioni ove opportuno\n",
    "        x = self.C3(x)\n",
    "        x = self.S4(x)\n",
    "        x = self.activation(x) # inseriamo le attivazioni ove opportuno\n",
    "\n",
    "        x = self.F5(x.view(x.shape[0], -1)) # dobbiamo effettuare un \"reshape\" del tensore\n",
    "        x = self.activation(x)\n",
    "        x = self.F6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.F7(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/10: 100%|██████████| 73/73 [00:05<00:00, 13.14batch/s, accuracy=0.0455, loss=3.1] \n",
      "Test Epoch 1/10: 100%|██████████| 31/31 [00:03<00:00,  9.07batch/s, accuracy=0.0384, loss=3.1]\n",
      "Train Epoch 2/10: 100%|██████████| 73/73 [00:06<00:00, 11.68batch/s, accuracy=0.0437, loss=3.09]\n",
      "Test Epoch 2/10: 100%|██████████| 31/31 [00:03<00:00,  9.30batch/s, accuracy=0.0384, loss=3.09]\n",
      "Train Epoch 3/10: 100%|██████████| 73/73 [00:04<00:00, 15.53batch/s, accuracy=0.148, loss=2.78] \n",
      "Test Epoch 3/10: 100%|██████████| 31/31 [00:07<00:00,  4.27batch/s, accuracy=0.386, loss=2.17]\n",
      "Train Epoch 4/10: 100%|██████████| 73/73 [00:04<00:00, 14.67batch/s, accuracy=0.461, loss=1.53]\n",
      "Test Epoch 4/10: 100%|██████████| 31/31 [00:03<00:00, 10.01batch/s, accuracy=0.631, loss=1.17]\n",
      "Train Epoch 5/10: 100%|██████████| 73/73 [00:04<00:00, 15.99batch/s, accuracy=0.652, loss=1.04] \n",
      "Test Epoch 5/10: 100%|██████████| 31/31 [00:04<00:00,  6.92batch/s, accuracy=0.738, loss=1.36]\n",
      "Train Epoch 6/10: 100%|██████████| 73/73 [00:04<00:00, 16.79batch/s, accuracy=0.23, loss=3.06] \n",
      "Test Epoch 6/10: 100%|██████████| 31/31 [00:03<00:00,  9.99batch/s, accuracy=0.0485, loss=3.15]\n",
      "Train Epoch 7/10: 100%|██████████| 73/73 [00:04<00:00, 15.06batch/s, accuracy=0.0528, loss=3.13]\n",
      "Test Epoch 7/10: 100%|██████████| 31/31 [00:03<00:00,  9.61batch/s, accuracy=0.0384, loss=3.14]\n",
      "Train Epoch 8/10: 100%|██████████| 73/73 [00:04<00:00, 16.18batch/s, accuracy=0.0485, loss=3.11]\n",
      "Test Epoch 8/10: 100%|██████████| 31/31 [00:03<00:00,  9.71batch/s, accuracy=0.0414, loss=3.11]\n",
      "Train Epoch 9/10: 100%|██████████| 73/73 [00:04<00:00, 16.15batch/s, accuracy=0.0511, loss=3.11]\n",
      "Test Epoch 9/10: 100%|██████████| 31/31 [00:03<00:00,  9.93batch/s, accuracy=0.0414, loss=3.11]\n",
      "Train Epoch 10/10: 100%|██████████| 73/73 [00:04<00:00, 15.17batch/s, accuracy=0.0463, loss=3.1] \n",
      "Test Epoch 10/10: 100%|██████████| 31/31 [00:03<00:00,  9.92batch/s, accuracy=0.0485, loss=3.1]\n"
     ]
    }
   ],
   "source": [
    "# Alleniamo il nuovo modello\n",
    "lenet_v2 = LeNetV2()\n",
    "lenet_v2 = train_classifier(lenet_v2, train_data, test_data, exp_name='lenet_v2', lr=0.01, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 31/31 [00:09<00:00,  3.33it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lenet_labels_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lenet_v2_predictions_test, lenet_v2_labels_test \u001b[38;5;241m=\u001b[39m test_classifier(lenet_v2, test_data)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrore LeNet su DIGITS training: \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(perc_error(lenet_labels_train, lenet_predictions_train)))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrore LeNetV2 su DIGITS training: \u001b[39m\u001b[38;5;132;01m%0.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(perc_error(lenet_labels_test, lenet_v2_predictions_test)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lenet_labels_train' is not defined"
     ]
    }
   ],
   "source": [
    "lenet_v2_predictions_test, lenet_v2_labels_test = test_classifier(lenet_v2, test_data)\n",
    "print(\"Errore LeNet su DIGITS training: %0.2f%%\"%(perc_error(lenet_labels_train, lenet_predictions_train)))\n",
    "print(\"Errore LeNetV2 su DIGITS training: %0.2f%%\"%(perc_error(lenet_labels_test, lenet_v2_predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lenet_v2, \"lenet_v2_lr01_ep10\" + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
