{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Funzioni globali / comuni a tutti i modelli",
   "id": "d6757943674b3ea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AverageValueMeter",
   "id": "21d8451bead02ac"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-13T14:40:43.064203Z",
     "start_time": "2024-07-13T14:40:43.061751Z"
    }
   },
   "source": [
    "class AverageValueMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "\n",
    "    def add(self, val, n=1):\n",
    "        self.sum += val*n\n",
    "        self.num += n\n",
    "\n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset train, validation, test",
   "id": "156b17b812bf7891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:59:16.081295Z",
     "start_time": "2024-07-13T18:59:16.013780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_train_val_test(dataset, perc=None): # default 60% train, 10% val, 30% test\n",
    "    if perc is None:\n",
    "        perc = [0.6, 0.1, 0.3]\n",
    "    train, testval = train_test_split(dataset, test_size=perc[1] + perc[2])\n",
    "    val, test = train_test_split(testval, test_size=perc[2] / (perc[1]+perc[2]))\n",
    "    return train, val, test"
   ],
   "id": "d705bc2a179ec40a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classe dataset che permette di caricare immagini dal disco a partire da un csv",
   "id": "e057d109c2b0f20d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "class CSVImageDataset(data.Dataset):\n",
    "    def __init__(self, data_root, csv, transform=None):\n",
    "        self.data_root = data_root\n",
    "        self.data = pd.read_csv(csv)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_path, im_label = self.data.iloc[idx]['path'], self.data.iloc[idx].label\n",
    "        # convertiamo tutto in RGB\n",
    "        im = Image.open(join(self.data_root, im_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        return im, im_label"
   ],
   "id": "acce7dd4303134ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Funzione per scaricare Squeezenet (adattabile ad altri modelli pre-trained)",
   "id": "7c045965c1ed1753"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T19:16:43.040142Z",
     "start_time": "2024-07-13T19:16:41.994677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from torchvision.models import squeezenet1_0\n",
    "from torchvision.models import SqueezeNet1_0_Weights\n",
    "\n",
    "def get_squeezenet_model(num_classes):\n",
    "    model = squeezenet1_0(weights = SqueezeNet1_0_Weights.DEFAULT)\n",
    "    num_classes = 101\n",
    "    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "    model.num_classes = num_classes\n",
    "    return model"
   ],
   "id": "7f99a26c53c0bc31",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Funzione per training, print epoche e generazioni grafici tensorboard (semplice)",
   "id": "9e7d33a32703abd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T14:45:27.027583Z",
     "start_time": "2024-07-13T14:45:27.022577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    loader = {\n",
    "        'train': train_loader,\n",
    "        'test': test_loader\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                with tqdm.tqdm(enumerate(loader[phase]), total=len(loader[phase]), desc=f\"{phase.capitalize()} Epoch {e+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "                    for i, batch in pbar:\n",
    "                        x = batch[0].to(device)\n",
    "                        y = batch[1].to(device)\n",
    "                        output = model(x)\n",
    "\n",
    "                        n = x.shape[0]\n",
    "                        global_step += n\n",
    "                        loss = criterion(output, y)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        accuracy = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
    "                        loss_meter.add(loss.item(), n)\n",
    "                        acc_meter.add(accuracy, n)\n",
    "\n",
    "                        pbar.set_postfix(loss=loss_meter.value(), accuracy=acc_meter.value())\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                            writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('loss/' + phase, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + phase, acc_meter.value(), global_step=global_step)\n",
    "\n",
    "        torch.save(model.state_dict(), '%s-%d.pth' % (exp_name, e+1))\n",
    "\n",
    "    return model"
   ],
   "id": "cb266ccf166a6b9c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T14:45:27.393854Z",
     "start_time": "2024-07-13T14:45:27.391275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "\n",
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm.tqdm(loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for batch in pbar:\n",
    "                x = batch[0].to(device)\n",
    "                y = batch[1].to(device)\n",
    "                output = model(x)\n",
    "                preds = output.to(\"cpu\").max(1)[1].numpy()\n",
    "                labs = y.to(\"cpu\").numpy()\n",
    "                predictions.extend(list(preds))\n",
    "                labels.extend(list(labs))\n",
    "\n",
    "    return np.array(predictions), np.array(labels)"
   ],
   "id": "f69f09c2026848af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Funzione per training, print epoche e generazioni grafici tensorboard (più complessa",
   "id": "7ab284dfaae0c21f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def trainval_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    # meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    # writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    # device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    loader = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "    global_step = 0\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch {e + 1} of {epochs}')\n",
    "        # iterazione tra train e test\n",
    "        for phase in ['train', 'test']:\n",
    "            loss_meter.reset()\n",
    "            acc_meter.reset()\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                with tqdm.tqdm(enumerate(loader[phase]), total=len(loader[phase]), desc=f\"{phase.capitalize()} Epoch {e + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "                    for i, batch in pbar:\n",
    "                        x = batch[0].to(device)\n",
    "                        y = batch[1].to(device)\n",
    "                        output = model(x)\n",
    "\n",
    "                        # global step update\n",
    "                        batch_elements = x.shape[0]\n",
    "                        global_step += batch_elements\n",
    "                        # loss\n",
    "                        loss = criterion(output, y)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        acc = accuracy_score(y.to('cpu'), output.to('cpu').max(1)[1])\n",
    "                        loss_meter.add(loss.item(), batch_elements)\n",
    "                        acc_meter.add(acc, batch_elements)\n",
    "\n",
    "                        pbar.set_postfix(loss=loss_meter.value(), accuracy=acc_meter.value())\n",
    "\n",
    "                        # log dei risultati di training\n",
    "                        if phase == 'train':\n",
    "                            writer.add_scalar('Loss/train', loss_meter.value(), global_step)\n",
    "                            writer.add_scalar('Accuracy/train', acc_meter.value(), global_step)\n",
    "            # epoca finita\n",
    "            writer.add_scalar('loss/' + phase, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + phase, acc_meter.value(), global_step=global_step)\n",
    "        torch.save(model.state_dict(), f'weights/{exp_name}-{e + 1}.pth')\n",
    "    return model"
   ],
   "id": "88260a01420c098b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T17:34:28.377849Z",
     "start_time": "2024-07-14T17:34:28.345632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_classifier_v2(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.nograd():\n",
    "        with tqdm.tqdm(loader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for batch in pbar:\n",
    "                x = batch[0].to(device)\n",
    "                y = batch[1].to(device)\n",
    "                output = model(x)\n",
    "                preds = output.to(\"cpu\").max(dim=1)[1]\n",
    "                labs = y.to(\"cpu\").numpy()\n",
    "                predictions.extend(list(preds))\n",
    "                labels.extend(list(labs))\n",
    "\n",
    "    return np.array(predictions), np.array(labels)"
   ],
   "id": "469d7cd8bb31756d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MiniAlexNet + DropOut (dal lab)\n",
    "- Nota: out_classes=100, nel nostro caso dovrebbe essere 22 per le lettere del dataset"
   ],
   "id": "dc8a5ead0400964d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch import nn\n",
    "class MiniAlexNet_Dropout(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNet_Dropout, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), # 3 canali in input, 16 mappe di feature in output, kernel 5x5 | input: 3x32x32 -> output: 16x32x32\n",
    "            nn.MaxPool2d(2), # Max pooling 2x2 | input: 32x32x32 -> output: 32x16x16\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, 5, padding=2), # 16 canali in input, 32 mappe di feature in output, kernel 5x5 | input: 16x16x16 -> output: 32x16x16\n",
    "            nn.MaxPool2d(2), # Max pooling 2x2 | input: 64x16x16 -> output: 64x8x8\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1), # 32 canali in input, 64 mappe di feature in output, kernel 3x3 | input: 32x8x8 -> output: 64x8x8\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1), # 64 canali in input, 128 mappe di feature in output, kernel 3x3 | input: 64x8x8 -> output: 128x8x8\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1), # 128 canali in input, 256 mappe di feature in output, kernel 3x3 | input: 128x8x8 -> output: 256x8x8\n",
    "            nn.MaxPool2d(2), # Max pooling 2x2 | input: 128x8x8 -> output: 128x4x4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*4*4, 2048), #input 256*4*4=4096, output 2048\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 1024), #input 2048, output 1024\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, out_classes) #input 1024, output 100\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0], -1))\n",
    "        return x"
   ],
   "id": "354c12683194ef8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# esempio di utilizzo\n",
    "mini_alexnet_dropout = MiniAlexNet_Dropout()\n",
    "# mini_alexnet_dropout = train_classifier(mini_alexnet_dropout, <DataLoader-di-Training>, <DataLoader-di-Testing>, exp_name='mini_alexnet_v2', lr=0.01, epochs=10)"
   ],
   "id": "ec5be086fc553ef2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
